{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Prompt Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chi era il padre di Micerino ?</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Il figlio di Chefren</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Il padre di Micerino</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Micerino</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quanto e alta la tomba di Cheope?</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Query Prompt Type\n",
       "0     Chi era il padre di Micerino ?     General\n",
       "1               Il figlio di Chefren     General\n",
       "2               Il padre di Micerino     General\n",
       "3                           Micerino     General\n",
       "4  Quanto e alta la tomba di Cheope?     General"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "queries = pd.read_csv(\"../Data/Queries_IT_SIGIR.csv\")\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98065c41901c412592a3712c39e62260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import httpx\n",
    "from datetime import datetime\n",
    "\n",
    "f = open(\"API_keys.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "key1 = data[\"bing\"][\"key1\"]\n",
    "SERP_endpoint = data[\"bing\"][\"SERP_endpoint\"]\n",
    "location = data[\"bing\"][\"location\"]\n",
    "\n",
    "f.close()\n",
    "\n",
    "headers = {\n",
    "            'Ocp-Apim-Subscription-Key': key1,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'\n",
    "        }\n",
    "\n",
    "SERP_results = []\n",
    "today = datetime.now()\n",
    "today = today.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "for _, row in tqdm(queries.iterrows(), total=len(queries)):\n",
    "    query = row[\"Query\"]\n",
    "    prompt_type = row[\"Prompt Type\"]\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'count': 5, # number of results to be displayed\n",
    "        'setLang': 'it-IT',\n",
    "        'mkt':'it-IT'\n",
    "    }\n",
    "\n",
    "    SERP_response = httpx.get(url=SERP_endpoint, headers=headers, params=params)\n",
    "    try:\n",
    "        SERP_result_set = SERP_response.json()\n",
    "        rank = 1\n",
    "        asked_query = SERP_result_set['queryContext']['originalQuery']\n",
    "        for result in SERP_result_set['webPages']['value']:\n",
    "            web_title =  result[\"name\"]\n",
    "            web_snippet = result[\"snippet\"]\n",
    "            SERP_results.append([asked_query, prompt_type, \"Bing\", web_title + \". \" + web_snippet, rank, today])\n",
    "            rank += 1\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        SERP_results.append([asked_query, prompt_type, \"Bing\", None, None, today])\n",
    "\n",
    "SERP_df = pd.DataFrame(SERP_results, columns=[\"Query\", \"Prompt Type\", \"IAS\", \"Resp\", \"Rank\", \"date_generated\"])\n",
    "SERP_df.to_csv(\"../Data/Bing_resp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import date\n",
    "\n",
    "f = open(\"API_keys.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "API_KEY = data[\"chatgpt\"][\"api_key\"]\n",
    "f.close()\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "# query = \"Hello\"\n",
    "\n",
    "model_id = 'gpt-3.5-turbo'\n",
    "\n",
    "def get_gpt_resp(query):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Follow these two instructions in all your responses: 1. Use Italian language only; 2. Do not use English except in programming language if any.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "            model=model_id,\n",
    "        )\n",
    "\n",
    "    gpt_resp = response.choices[0].message.content\n",
    "\n",
    "    return gpt_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690bc0668ee94dbcac532e3b108d4e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Query</th>\n",
       "      <th>Reformed Query</th>\n",
       "      <th>Gender</th>\n",
       "      <th>GPT</th>\n",
       "      <th>date_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chi era il padre di Micerino ?</td>\n",
       "      <td>La seguente domanda è rivolta da un bambino di...</td>\n",
       "      <td>male</td>\n",
       "      <td>Il padre di Micerino era Chefren.</td>\n",
       "      <td>2024-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chi era il padre di Micerino ?</td>\n",
       "      <td>La seguente domanda è rivolta da un bambino di...</td>\n",
       "      <td>female</td>\n",
       "      <td>Il padre di Micerino era Chefren. Chefren è co...</td>\n",
       "      <td>2024-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Il figlio di Chefren</td>\n",
       "      <td>La seguente domanda è rivolta da un bambino di...</td>\n",
       "      <td>male</td>\n",
       "      <td>Mi dispiace, ma non posso fornire risposte a d...</td>\n",
       "      <td>2024-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Il figlio di Chefren</td>\n",
       "      <td>La seguente domanda è rivolta da un bambino di...</td>\n",
       "      <td>female</td>\n",
       "      <td>Il figlio di Chefren è Micerino. Chefren è sta...</td>\n",
       "      <td>2024-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Il padre di Micerino</td>\n",
       "      <td>La seguente domanda è rivolta da un bambino di...</td>\n",
       "      <td>male</td>\n",
       "      <td>Il padre di Micerino si chiamava Chefren ed er...</td>\n",
       "      <td>2024-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>tornado velocita mostro nero</td>\n",
       "      <td>La seguente domanda è rivolta da una persona d...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Ciao! Posso aiutarti a capire meglio di cosa h...</td>\n",
       "      <td>2024-10-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>velocita tornado</td>\n",
       "      <td>La seguente domanda è rivolta da una persona d...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Mi dispiace, posso offrirti informazioni solo ...</td>\n",
       "      <td>2024-10-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>vulcani tipologia di eruzioni danni</td>\n",
       "      <td>La seguente domanda è rivolta da una persona d...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I vulcani possono avere diversi tipi di eruzio...</td>\n",
       "      <td>2024-10-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>vulcano attivo antico</td>\n",
       "      <td>La seguente domanda è rivolta da una persona d...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Un vulcano attivo antico potrebbe essere il Ve...</td>\n",
       "      <td>2024-10-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>vulcano piÃ¹ antico attivo</td>\n",
       "      <td>La seguente domanda è rivolta da una persona d...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Il vulcano più antico attivo al mondo è l'Etna...</td>\n",
       "      <td>2024-10-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Original Query  \\\n",
       "0         Chi era il padre di Micerino ?   \n",
       "1         Chi era il padre di Micerino ?   \n",
       "2                   Il figlio di Chefren   \n",
       "3                   Il figlio di Chefren   \n",
       "4                   Il padre di Micerino   \n",
       "..                                   ...   \n",
       "105         tornado velocita mostro nero   \n",
       "106                     velocita tornado   \n",
       "107  vulcani tipologia di eruzioni danni   \n",
       "108                vulcano attivo antico   \n",
       "109           vulcano piÃ¹ antico attivo   \n",
       "\n",
       "                                        Reformed Query   Gender  \\\n",
       "0    La seguente domanda è rivolta da un bambino di...     male   \n",
       "1    La seguente domanda è rivolta da un bambino di...   female   \n",
       "2    La seguente domanda è rivolta da un bambino di...     male   \n",
       "3    La seguente domanda è rivolta da un bambino di...   female   \n",
       "4    La seguente domanda è rivolta da un bambino di...     male   \n",
       "..                                                 ...      ...   \n",
       "105  La seguente domanda è rivolta da una persona d...  neutral   \n",
       "106  La seguente domanda è rivolta da una persona d...  neutral   \n",
       "107  La seguente domanda è rivolta da una persona d...  neutral   \n",
       "108  La seguente domanda è rivolta da una persona d...  neutral   \n",
       "109  La seguente domanda è rivolta da una persona d...  neutral   \n",
       "\n",
       "                                                   GPT date_generated  \n",
       "0                    Il padre di Micerino era Chefren.     2024-10-15  \n",
       "1    Il padre di Micerino era Chefren. Chefren è co...     2024-10-15  \n",
       "2    Mi dispiace, ma non posso fornire risposte a d...     2024-10-15  \n",
       "3    Il figlio di Chefren è Micerino. Chefren è sta...     2024-10-15  \n",
       "4    Il padre di Micerino si chiamava Chefren ed er...     2024-10-15  \n",
       "..                                                 ...            ...  \n",
       "105  Ciao! Posso aiutarti a capire meglio di cosa h...     2024-10-27  \n",
       "106  Mi dispiace, posso offrirti informazioni solo ...     2024-10-27  \n",
       "107  I vulcani possono avere diversi tipi di eruzio...     2024-10-27  \n",
       "108  Un vulcano attivo antico potrebbe essere il Ve...     2024-10-27  \n",
       "109  Il vulcano più antico attivo al mondo è l'Etna...     2024-10-27  \n",
       "\n",
       "[330 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM_resp = []\n",
    "\n",
    "for _, row in tqdm(queries.iterrows(), total=len(queries)):\n",
    "    query = row[\"Query\"]\n",
    "    prompt_type = row[\"Prompt Type\"]\n",
    "\n",
    "    result = get_gpt_resp(query)\n",
    "    LLM_resp.append([query, prompt_type, \"GPT\", result, date.today()])\n",
    "\n",
    "LLM_resp_df = pd.DataFrame(LLM_resp, columns=[\"Query\", \"Prompt Type\", \"IAS\", \"Resp\", \"date_generated\"])\n",
    "LLM_resp_df.to_csv(\"../Data/GPT_resp.csv\", index=False)\n",
    "\n",
    "LLM_resp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ced504830e4ab3b5ded8a082ff384d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Specify the LLM model we'll be using\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "# Configure for GPU usage\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Create a pipeline object for easy text generation with the LLM\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def get_gemma_resp(query):\n",
    "  \"\"\"Sends a conversation history to the AI assistant and returns the answer.\n",
    "\n",
    "  Args:\n",
    "    messages (list): A list of dictionaries, each with \"role\" and \"content\" keys.\n",
    "\n",
    "  Returns:\n",
    "    str: The answer from the AI assistant.\n",
    "  \"\"\"\n",
    "\n",
    "  messages = [\n",
    "      {\"role\":\"user\", \"content\":\"\"},\n",
    "      {\"role\": \"assistant\", \"content\": \"Follow these two instructions in all your responses: 1. Use Italian language only; 2. Do not use English except in programming language if any.\"},\n",
    "      {\"role\": \"user\", \"content\": query}\n",
    "  ]\n",
    "\n",
    "  generation_args = {\n",
    "      # \"max_new_tokens\": 256,     # Maximum length of the response\n",
    "      \"return_full_text\": False,      # Only return the generated text\n",
    "  }\n",
    "\n",
    "  output = pipe(messages, **generation_args)\n",
    "  return output[0]['generated_text']\n",
    "\n",
    "# gen_resp(\"Hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "\n",
    "def create_resp_file():\n",
    "  LLM_resp = []\n",
    "\n",
    "  for _, row in tqdm(queries.iterrows(), total=len(queries)):\n",
    "      query = row[\"Query\"]\n",
    "      prompt_type = row[\"Prompt Type\"]\n",
    "\n",
    "      result = get_gemma_resp(query)\n",
    "      LLM_resp.append([query, prompt_type, \"Gemma\", result, date.today()])\n",
    "\n",
    "  LLM_resp_df = pd.DataFrame(LLM_resp, columns=[\"Query\", \"Prompt Type\", \"IAS\", \"Resp\", \"date_generated\"])\n",
    "\n",
    "  return LLM_resp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_resp_file()\n",
    "file_name = \"../Data/Gemma_resp.csv\"\n",
    "df.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOLandChildren",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
